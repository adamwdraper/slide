---
title: 'Agent Evaluations'
description: 'Test and evaluate your Tyler agents with confidence'
---

## Overview

Tyler's evaluation framework provides a powerful, agent-focused testing system that helps you ensure your agents behave correctly across various scenarios. Built on top of Weave, it offers intuitive APIs for defining test conversations, setting expectations, and measuring agent performance.

<Note>
  The evaluation framework uses **mock tools by default** to ensure safe testing without making real API calls or affecting external systems.
</Note>

## Key Features

<CardGroup cols={2}>
  <Card title="Conversation Testing" icon="comments">
    Test single or multi-turn conversations with complex scenarios
  </Card>
  <Card title="Flexible Expectations" icon="check-circle">
    Define precise expectations for agent behavior and responses
  </Card>
  <Card title="Multiple Scorers" icon="chart-line">
    Evaluate tone, task completion, tool usage, and conversation flow
  </Card>
  <Card title="Mock Tools" icon="shield">
    Safe testing with automatic tool mocking to prevent side effects
  </Card>
</CardGroup>

## Quick Start

### Basic Evaluation

```python
from tyler import Agent
from tyler.eval import (
    AgentEval, 
    Conversation, 
    Expectation,
    ToolUsageScorer,
    ToneScorer
)
from lye import WEB_TOOLS, FILES_TOOLS

# Create your agent
agent = Agent(
    name="assistant",
    model_name="gpt-4",
    purpose="To be a helpful assistant",
    tools=[*WEB_TOOLS, *FILES_TOOLS]
)

# Define test conversations
conversations = [
    Conversation(
        id="greeting_test",
        user="Hello, how are you?",
        expect=Expectation(
            tone="friendly",
            mentions_any=["hello", "hi", "greetings"],
            does_not_use_tools=["web-search", "files-write"]
        )
    )
]

# Create evaluation
eval = AgentEval(
    name="basic_agent_test",
    conversations=conversations,
    scorers=[ToneScorer(), ToolUsageScorer()]
)

# Run evaluation (with mock tools by default)
results = await eval.run(agent)
print(results.summary())
```

## Core Concepts

### Conversations

Define test scenarios using the `Conversation` class:

```python
# Single-turn conversation
Conversation(
    id="weather_query",
    user="What's the weather in NYC?",
    expect=Expectation(
        uses_tools=["web-search"],
        mentions=["weather", "NYC"]
    )
)

# Multi-turn conversation
Conversation(
    id="booking_flow",
    turns=[
        Turn(role="user", content="I need to book a flight"),
        Turn(role="assistant", expect=Expectation(
            asks_clarification=True
        )),
        Turn(role="user", content="From NYC to London next week"),
        Turn(role="assistant", expect=Expectation(
            uses_tools=["web-search"],
            confirms_details=["NYC", "London"]
        ))
    ]
)
```

### Expectations

Set precise expectations for agent behavior:

#### Tool Usage Expectations

```python
Expectation(
    # Exact tools that should be used
    uses_tools=["web-search", "files-write"],
    
    # At least one of these tools should be used
    uses_any_tool=["web-search", "web-fetch"],
    
    # Tools must be used in this order
    uses_tools_in_order=["web-search", "files-write"],
    
    # Tools that should NOT be used
    does_not_use_tools=["files-delete"]
)
```

<Warning>
  When referencing Lye tools in expectations, use their registered names:
  - `"web-search"` not `"web.search"`
  - `"files-write"` not `"files.write"`  
  - `"audio-transcribe"` not `"audio.transcribe"`
</Warning>

#### Content Expectations

```python
Expectation(
    # Must mention all of these
    mentions=["Paris", "Friday"],
    
    # Must mention at least one
    mentions_any=["date", "when", "time"],
    
    # Must not mention any
    mentions_none=["error", "failed", "sorry"]
)
```

#### Behavioral Expectations

```python
Expectation(
    # Agent should ask for more information
    asks_clarification=True,
    
    # Agent should confirm specific details
    confirms_details=["flight", "destination"],
    
    # Agent should suggest alternatives
    offers_alternatives=True,
    
    # Agent should refuse the request
    refuses_request=True,
    
    # Agent should complete the task
    completes_task=True
)
```

#### Custom Validation

```python
Expectation(
    # Custom validation function
    custom=lambda response: "booking confirmed" in response["content"].lower()
)
```

### Scorers

Tyler provides several built-in scorers:

#### ToolUsageScorer

Validates tool usage against expectations:

```python
ToolUsageScorer(strict=True)  # Strict mode fails if any tool expectation is violated
```

#### ToneScorer

Uses LLM to evaluate response tone:

```python
ToneScorer(
    acceptable_tones=["friendly", "professional", "helpful"],
    model="gpt-4"
)
```

#### TaskCompletionScorer

Evaluates whether the agent completed the requested task:

```python
TaskCompletionScorer(model="gpt-4")
```

#### ConversationFlowScorer

Assesses natural conversation flow and coherence:

```python
ConversationFlowScorer(model="gpt-4")
```

## Advanced Usage

### Multiple Trials

Run each conversation multiple times to test consistency:

```python
eval = AgentEval(
    name="consistency_test",
    conversations=conversations,
    scorers=scorers,
    trials=3  # Run each conversation 3 times
)
```

### Working with Results

```python
# Get overall statistics
print(f"Pass rate: {results.pass_rate:.2%}")
print(f"Average score: {results.average_score:.2f}")

# Analyze failed conversations
for conv in results.get_failed_conversations():
    print(f"\nFailed: {conv.conversation_id}")
    for score in conv.get_failed_scores():
        print(f"  - {score.name}: {score.details}")

# Get specific conversation results
weather_result = results.get_conversation("weather_query")
if weather_result:
    print(f"Tool usage score: {weather_result.get_score('tool_usage')}")
```

### Score Breakdown

```python
# Get summary by scorer
for scorer_name, stats in results.score_summary().items():
    print(f"{scorer_name}:")
    print(f"  Average: {stats['average_score']:.2f}")
    print(f"  Passed: {stats['passed']}/{stats['total']}")
```

## Mock Tools

By default, evaluations use mock tools to ensure safety:

### How Mock Tools Work

1. **Automatic Mocking**: All agent tools are automatically replaced with mocks
2. **Realistic Responses**: Mocks return realistic data based on tool type
3. **Call Recording**: All mock tool calls are recorded for analysis
4. **No Side Effects**: No real API calls, file writes, or external changes

### Using Real Tools (Dangerous!)

<Warning>
  Using real tools in evaluations can result in actual API calls, file modifications, and other side effects. Only do this if absolutely necessary!
</Warning>

```python
# ⚠️ DANGER: This will use real tools!
eval = AgentEval(
    name="dangerous_real_test",
    conversations=conversations,
    scorers=scorers,
    use_mock_tools=False  # ⚠️ Real tools!
)
```

### Custom Mock Responses

```python
from tyler.eval import MockTool, mock_success

# Create custom mock
custom_mock = MockTool(
    original_tool=web.search,
    response=mock_success({"results": ["Custom result 1", "Custom result 2"]})
)
```

## Integration with Weave

Tyler's evaluation framework is built on Weave, allowing you to:

1. **Track Experiments**: All evaluations are logged to Weave
2. **Compare Runs**: Easily compare different agent configurations
3. **Visualize Results**: Use Weave's UI to analyze performance
4. **Export Data**: Access raw evaluation data for custom analysis

```python
import weave

# Initialize Weave project
weave.init("my-agent-evals")

# Run evaluation (automatically tracked)
results = await eval.run(agent)

# Results include Weave run ID
print(f"Weave run: {results.weave_run_id}")
```

## Best Practices

<AccordionGroup>
  <Accordion title="Write Comprehensive Test Scenarios">
    - Test both happy paths and edge cases
    - Include multi-turn conversations for complex flows
    - Test error handling and edge cases
  </Accordion>
  
  <Accordion title="Use Specific Expectations">
    - Be precise about what you expect
    - Combine multiple expectation types for thorough testing
    - Use custom validators for complex logic
  </Accordion>
  
  <Accordion title="Always Use Mock Tools First">
    - Start with mock tools to ensure safety
    - Only use real tools when absolutely necessary
    - Record and analyze mock tool calls
  </Accordion>
  
  <Accordion title="Run Multiple Trials">
    - Test consistency with multiple trials
    - Look for non-deterministic behavior
    - Calculate confidence intervals
  </Accordion>
  
  <Accordion title="Analyze Failures Thoroughly">
    - Examine failed scores in detail
    - Review agent responses that didn't meet expectations
    - Iterate on your agent based on results
  </Accordion>
</AccordionGroup>

## Complete Example

Here's a comprehensive example testing a travel booking agent:

```python
from tyler import Agent
from tyler.eval import (
    AgentEval, Conversation, Turn, Expectation,
    ToolUsageScorer, ToneScorer, TaskCompletionScorer, ConversationFlowScorer
)
from lye import WEB_TOOLS, FILES_TOOLS

# Create agent
agent = Agent(
    name="travel_assistant",
    model_name="gpt-4",
    purpose="Help users book flights and plan trips",
    tools=[*WEB_TOOLS, *FILES_TOOLS]
)

# Define comprehensive test suite
conversations = [
    # Test clarification requests
    Conversation(
        id="incomplete_request",
        user="Book me a flight to Paris",
        expect=Expectation(
            asks_clarification=True,
            mentions_any=["when", "date", "departure"],
            does_not_use_tools=["web-search"],  # Should ask first
            tone="helpful"
        )
    ),
    
    # Test complete booking flow
    Conversation(
        id="full_booking",
        turns=[
            Turn(role="user", content="I need a flight from NYC to London"),
            Turn(role="assistant", expect=Expectation(
                asks_clarification=True,
                confirms_details=["NYC", "London"]
            )),
            Turn(role="user", content="Next Tuesday, returning Sunday"),
            Turn(role="assistant", expect=Expectation(
                uses_tools=["web-search"],
                mentions=["Tuesday", "Sunday"],
                completes_task=True
            ))
        ]
    ),
    
    # Test edge cases
    Conversation(
        id="impossible_request",
        user="Book a flight to the moon",
        expect=Expectation(
            refuses_request=True,
            offers_alternatives=True,
            tone="helpful",
            mentions_any=["not possible", "cannot", "unable"]
        )
    ),
    
    # Test tool chaining
    Conversation(
        id="search_and_save",
        user="Find flights to Tokyo next month and save the results to a file",
        expect=Expectation(
            uses_tools_in_order=["web-search", "files-write"],
            mentions=["Tokyo", "saved", "file"],
            completes_task=True
        )
    )
]

# Create evaluation
eval = AgentEval(
    name="travel_agent_comprehensive",
    description="Full test suite for travel booking agent",
    conversations=conversations,
    scorers=[
        ToolUsageScorer(strict=True),
        ToneScorer(acceptable_tones=["helpful", "professional", "friendly"]),
        TaskCompletionScorer(),
        ConversationFlowScorer()
    ],
    trials=2  # Run each test twice
)

# Run evaluation
results = await eval.run(agent)

# Analyze results
print(f"Overall pass rate: {results.pass_rate:.2%}")
print(f"Average score: {results.average_score:.2f}")

# Detailed analysis
if results.failed_conversations:
    print("\nFailed tests:")
    for conv in results.get_failed_conversations():
        print(f"\n{conv.conversation_id}:")
        print(f"  Agent said: {conv.agent_response['content'][:100]}...")
        for score in conv.get_failed_scores():
            print(f"  ❌ {score.name}: {score.details.get('reasoning', 'No details')}")
```

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Agent Examples"
    icon="code"
    href="/packages/tyler/examples"
  >
    See more agent examples
  </Card>
  <Card
    title="Custom Scorers"
    icon="chart-line"
    href="/api-reference/tyler/eval/scorers"
  >
    Create custom scoring logic
  </Card>
  <Card
    title="Weave Integration"
    icon="wave"
    href="https://docs.wandb.ai/guides/weave"
  >
    Learn more about Weave
  </Card>
  <Card
    title="Best Practices"
    icon="star"
    href="/concepts/agents#testing"
  >
    Agent testing guidelines
  </Card>
</CardGroup> 