# Tyler Streaming API - Executive Summary

## TL;DR

Tyler's streaming API is **90% compliant** with OpenAI Agents SDK and LiteLLM standards. The main gap is **thinking/reasoning token support** in event mode (though it works in raw mode, undocumented).

**Quick fix:** Document that raw mode preserves thinking tokens (1-3 days)  
**Full fix:** Add `LLM_THINKING_CHUNK` event type (2-3 weeks)

---

## Current State

### What Works Well ‚úÖ
- Content streaming (token-by-token)
- Tool usage tracking (better than OpenAI SDK!)
- Raw streaming mode (OpenAI compatible)
- Comprehensive error handling
- Multi-turn iteration with tools

### What's Missing ‚ùå
1. **Thinking tokens in event mode** (works in raw mode, just undocumented)
2. **`stream_options` parameter** (for usage timing control)
3. **Finish reason events** (tracked but not exposed)
4. **Agent state change events** (for handoff tracking)

---

## The Thinking Token Gap

### What Are Thinking Tokens?

Models like OpenAI o1 and Anthropic Claude emit their reasoning process as separate tokens:

```python
# Model output has TWO parts:
thinking/reasoning: "Let me break this down step by step..."
content: "The answer is 42"
```

### Tyler Currently (Event Mode)
```python
# Everything gets lumped together
async for event in agent.go(thread, stream=True):
    if event.type == EventType.LLM_STREAM_CHUNK:
        # Prints BOTH thinking and content mixed:
        # "Let me break this down... The answer is 42"
        print(event.data['content_chunk'], end="")
```

‚ùå **Problem:** No way to distinguish reasoning from response

### Tyler Currently (Raw Mode)
```python
# Actually works! Just undocumented
async for chunk in agent.go(thread, stream="raw"):
    delta = chunk.choices[0].delta
    
    if hasattr(delta, 'thinking'):  # Anthropic
        print(f"üí≠ {delta.thinking}")
    
    if hasattr(delta, 'reasoning_content'):  # OpenAI o1
        print(f"üß† {delta.reasoning_content}")
    
    if hasattr(delta, 'content'):
        print(delta.content, end="")
```

‚úÖ **Already works!** Just needs documentation

### Tyler Recommended (Event Mode)
```python
# Clear separation with new event type
async for event in agent.go(thread, stream=True):
    if event.type == EventType.LLM_THINKING_CHUNK:  # NEW
        print(f"üí≠ {event.data['thinking_chunk']}")
    
    elif event.type == EventType.LLM_STREAM_CHUNK:
        print(event.data['content_chunk'], end="")
```

‚úÖ **Clear distinction** between reasoning and response

---

## Comparison with Standards

### OpenAI Agents SDK
| Feature | OpenAI SDK | Tyler |
|---------|-----------|-------|
| Raw streaming | ‚úÖ RawResponsesStreamEvent | ‚úÖ `stream="raw"` |
| Higher-level events | ‚úÖ RunItemStreamEvent | ‚úÖ ExecutionEvent |
| Thinking tokens | ‚úÖ Yes | üü° Raw mode only |
| Tool tracking | ‚úÖ Basic | ‚úÖ **Better!** (duration, errors) |
| Agent events | ‚úÖ AgentUpdatedStreamEvent | ‚ùå Missing |

**Tyler advantage:** More granular tool observability!

### LiteLLM
| Feature | LiteLLM | Tyler |
|---------|---------|-------|
| Basic streaming | ‚úÖ Yes | ‚úÖ Yes |
| `stream_options` | ‚úÖ Yes | ‚ùå Missing |
| Thinking fields | ‚úÖ Passes through | ‚úÖ Passes through (raw mode) |
| Provider support | ‚úÖ 15+ providers | ‚úÖ Same (uses LiteLLM) |
| Event abstraction | ‚ùå No | ‚úÖ Yes (Tyler's strength) |

**Tyler advantage:** High-level event API for observability!

---

## Why This Matters

### Use Case 1: Transparent AI
```python
# User wants to see how the AI is thinking
User: "Should I buy this stock?"

AI Thinking: "I need to consider: P/E ratio, market trends, risk..."
AI Response: "Based on analysis, I recommend..."
```

**Without thinking events:** User only sees the recommendation  
**With thinking events:** User sees the reasoning process

### Use Case 2: Debugging Agents
```python
# Developer needs to debug why agent made a decision
async for event in agent.go(thread, stream=True):
    if event.type == EventType.LLM_THINKING_CHUNK:
        logger.debug(f"Agent reasoning: {event.data['thinking_chunk']}")
```

**Impact:** Better debugging and trust in AI decisions

### Use Case 3: Cost Optimization
```python
# Different token pricing for thinking vs output
agent = Agent(
    name="cost-aware",
    model_name="gpt-4.1",
    stream_options={"include_usage": True}  # Get usage early
)

# Can abort early if cost too high
async for event in agent.go(thread, stream=True):
    if event.type == EventType.LLM_RESPONSE:
        if event.data['tokens']['total_tokens'] > budget:
            break
```

---

## Recommendations

### Option 1: Quick Documentation Fix (3 days) ‚≠êÔ∏è RECOMMENDED
**What:** Document that raw mode already supports thinking tokens  
**Effort:** 3 days  
**Files:** 
- Update `docs/guides/streaming-responses.mdx`
- Add `examples/006_thinking_tokens_raw.py`
- Add test in `tests/models/test_thinking_raw.py`

**Pros:**
- ‚úÖ Unblocks users TODAY
- ‚úÖ No code changes
- ‚úÖ Zero risk

**Cons:**
- üü° Raw mode only (more complex for users)
- üü° Not as clean as event mode

### Option 2: Full Implementation (2-3 weeks)
**What:** Add thinking token support to event mode  
**Effort:** 2-3 weeks  
**Changes:**
1. Add `EventType.LLM_THINKING_CHUNK`
2. Update `_go_stream()` to emit thinking events
3. Add `stream_options` parameter
4. Add `EventType.LLM_FINISH` and `EventType.AGENT_UPDATED`
5. Comprehensive tests and docs

**Pros:**
- ‚úÖ Full standard compliance
- ‚úÖ Clean event API
- ‚úÖ Future-proof

**Cons:**
- üü° Takes 2-3 weeks
- üü° More testing needed

### Option 3: Hybrid (1 week) ‚≠êÔ∏è BEST VALUE
**What:** Quick doc fix + partial implementation  
**Week 1:** Document raw mode (3 days) + Add thinking events (2 days)  
**Result:** Users can use thinking tokens in BOTH modes

**Pros:**
- ‚úÖ Quick unblock (3 days)
- ‚úÖ Better DX with events (7 days)
- ‚úÖ Balanced effort

**Cons:**
- üü° Still missing stream_options and other events

---

## Effort Breakdown

### Quick Fix (Option 1)
```
Day 1: Write documentation for raw mode thinking
Day 2: Create example (006_thinking_tokens_raw.py)
Day 3: Add tests, review, merge
```
**Total: 3 days**

### Thinking Events Only (Option 3 - Week 1)
```
Day 1-3: Quick fix (documentation)
Day 4: Add LLM_THINKING_CHUNK event type
Day 5: Update _go_stream to emit thinking events
Day 6: Add tests for Anthropic + OpenAI o1
Day 7: Update docs, create example
```
**Total: 1 week**

### Full Implementation (Option 2)
```
Week 1: Thinking events (as above)
Week 2: stream_options + finish events
Week 3: Agent events + polish
Week 4: Documentation overhaul + examples
```
**Total: 3-4 weeks**

---

## Risk Assessment

### Option 1 (Documentation) - Risk: LOW ‚úÖ
- No code changes
- No regression risk
- No breaking changes
- Can be done in parallel with other work

### Option 2/3 (Implementation) - Risk: LOW-MEDIUM üü°
- Additive changes only (no breaking changes)
- New event types won't affect existing code
- Backward compatible
- Main risk: Testing with different providers

**Mitigation:**
- Comprehensive tests with mock responses
- Test with real o1 and Claude models
- Feature flag for gradual rollout (optional)

---

## Code Changes Summary

### Files to Modify
1. **`tyler/models/execution.py`** (10 lines)
   - Add 3 new event types

2. **`tyler/models/agent.py`** (50 lines)
   - Add `stream_options` field (5 lines)
   - Update `_go_stream` to handle thinking (30 lines)
   - Pass stream_options to LiteLLM (5 lines)
   - Emit finish events (10 lines)

3. **`tests/models/test_agent_streaming.py`** (100 lines)
   - Test thinking with Anthropic (30 lines)
   - Test reasoning with OpenAI o1 (30 lines)
   - Test stream_options (20 lines)
   - Test finish events (20 lines)

4. **`docs/guides/streaming-responses.mdx`** (200 lines)
   - Add thinking tokens section
   - Add examples
   - Update API reference

5. **`examples/`** (3 new files)
   - `006_thinking_tokens_raw.py`
   - `007_thinking_tokens_events.py`
   - `008_stream_options.py`

**Total: ~360 lines of new code**

---

## Decision Matrix

| Criteria | Option 1 (Doc) | Option 3 (Hybrid) | Option 2 (Full) |
|----------|----------------|-------------------|-----------------|
| **Time to value** | 3 days ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è | 1 week ‚≠êÔ∏è‚≠êÔ∏è | 3 weeks ‚≠êÔ∏è |
| **User experience** | Medium | High ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è | High ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è |
| **Standard compliance** | Partial | Good ‚≠êÔ∏è‚≠êÔ∏è | Full ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è |
| **Risk** | Very Low ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è | Low ‚≠êÔ∏è‚≠êÔ∏è | Low ‚≠êÔ∏è‚≠êÔ∏è |
| **Effort** | 3 days ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è | 1 week ‚≠êÔ∏è‚≠êÔ∏è | 3 weeks ‚≠êÔ∏è |
| **Maintenance** | None ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è | Low ‚≠êÔ∏è‚≠êÔ∏è | Medium ‚≠êÔ∏è |

---

## Recommendation

### üéØ Recommended: Option 3 (Hybrid Approach)

**Week 1 Actions:**
1. **Days 1-3:** Document raw mode thinking (quick unblock)
2. **Days 4-5:** Implement `LLM_THINKING_CHUNK` event
3. **Days 6-7:** Tests and examples

**Result:**
- Users can use thinking tokens TODAY (raw mode)
- Better UX available in 1 week (event mode)
- 90% of value for 25% of effort

**Then decide:**
- If thinking events prove valuable ‚Üí continue to full implementation
- If raw mode is sufficient ‚Üí stop here

---

## Sample Code: Before/After

### Before (Current - Event Mode)
```python
# Mixed content - can't distinguish thinking from response
async for event in agent.go(thread, stream=True):
    if event.type == EventType.LLM_STREAM_CHUNK:
        print(event.data['content_chunk'], end="")
        # Prints: "Let me think... The answer is 42"
```

### After Quick Fix (Raw Mode - Available TODAY after docs)
```python
# Can distinguish - but more complex
async for chunk in agent.go(thread, stream="raw"):
    delta = chunk.choices[0].delta
    if hasattr(delta, 'thinking'):
        print(f"üí≠ {delta.thinking}")
    if hasattr(delta, 'content'):
        print(delta.content, end="")
    # Output:
    # üí≠ Let me think...
    # The answer is 42
```

### After Full Fix (Event Mode - 1 week)
```python
# Clean API - best DX
async for event in agent.go(thread, stream=True):
    if event.type == EventType.LLM_THINKING_CHUNK:
        print(f"üí≠ {event.data['thinking_chunk']}")
    elif event.type == EventType.LLM_STREAM_CHUNK:
        print(event.data['content_chunk'], end="")
    # Output:
    # üí≠ Let me think...
    # The answer is 42
```

---

## Next Steps

### If Approved: Hybrid Approach (Option 3)

**This Week:**
1. Create spec document (following Slide workflow)
2. Write impact analysis
3. Draft TDR
4. Get approval

**Next Week (Implementation):**
1. Document raw mode thinking (Mon-Wed)
2. Implement thinking events (Thu-Fri)
3. Tests and examples (Weekend/Mon)

**Success Metrics:**
- [ ] Documentation published
- [ ] Example working with o1 model
- [ ] Example working with Claude model  
- [ ] Tests passing with both providers
- [ ] Zero breaking changes

---

## Questions for Discussion

1. **Urgency:** Do users need thinking tokens immediately?
   - If yes ‚Üí Start with Option 1 (quick doc)
   - If no ‚Üí Go straight to Option 3 (hybrid)

2. **Models:** Which models are priority?
   - OpenAI o1? (reasoning_content)
   - Anthropic Claude? (thinking)
   - Both?

3. **Scope:** Should we include other features?
   - stream_options?
   - Finish events?
   - Agent events?
   - Or just thinking for now?

4. **Process:** Follow Slide workflow?
   - Create spec ‚Üí impact ‚Üí TDR first?
   - Or fast-track documentation only?

---

## Appendix: Field Reference

### Thinking-Related Fields by Provider

```python
# OpenAI o1
chunk.choices[0].delta.reasoning_content  # String: reasoning process

# Anthropic Claude  
chunk.choices[0].delta.thinking  # String: thinking process

# Extended thinking (various)
chunk.choices[0].delta.extended_thinking  # String: deep reasoning

# Structured blocks (Anthropic)
chunk.thinking_blocks  # Array: [{type, content}, ...]
```

### Usage Fields

```python
# Standard usage (final chunk)
chunk.usage.prompt_tokens
chunk.usage.completion_tokens  
chunk.usage.total_tokens

# With stream_options (earlier in stream)
completion(
    stream=True,
    stream_options={"include_usage": True}
)
```

### Finish Reasons

```python
chunk.choices[0].finish_reason
# Values: "stop", "tool_calls", "length", "content_filter", "function_call"
```

