# Spec

**Feature name**: A2A Token-Level Streaming  
**One-line summary**: Enable real-time token streaming when Tyler agents are exposed via A2A protocol, so clients see responses as they're generated.

---

## Problem
When Tyler agents are exposed as A2A servers, clients must wait for the entire agent response to complete before receiving any output. This creates a poor user experience for long-running responses and doesn't leverage the A2A protocol's SSE streaming capabilities that Tyler already declares support for (`capabilities.streaming: true`).

Tyler already has excellent streaming infrastructure via `agent.stream()` that yields `ExecutionEvent` objects including `LLM_STREAM_CHUNK` for real-time token deliveryâ€”but this isn't wired into the A2A server.

## Goal
A2A clients connecting to Tyler A2A servers receive response tokens in real-time as they're generated by the LLM, using the protocol's `TaskArtifactUpdateEvent` streaming mechanism.

## Success Criteria
- [ ] A2A clients receive token chunks within ~100ms of generation (not waiting for full response)
- [ ] Streaming behavior is opt-in via server configuration (default: enabled)
- [ ] Non-streaming clients still work correctly (graceful degradation)
- [ ] No regression in non-streaming A2A functionality

## User Story
As a developer integrating with a Tyler A2A server, I want to receive response tokens in real-time as they're generated, so that my users see a responsive, streaming experience rather than waiting for the complete response.

## Flow / States

**Happy Path:**
1. Client sends `message/stream` request to Tyler A2A server
2. Server creates task, emits `TaskStatusUpdateEvent` (state: `working`)
3. Server executes Tyler agent with `agent.stream()`
4. For each `LLM_STREAM_CHUNK` event, server emits `TaskArtifactUpdateEvent` with `append=True`
5. On completion, server emits final `TaskArtifactUpdateEvent` with `lastChunk=True`
6. Server emits `TaskStatusUpdateEvent` (state: `completed`, `final=True`)

**Edge Case - Tool Calls:**
1. When agent invokes tools, streaming pauses
2. Server emits `TaskStatusUpdateEvent` with working status
3. After tool execution, streaming resumes with new LLM response
4. Multiple artifact updates may occur across tool iterations

## UX Links
- A2A Protocol Streaming Spec: https://a2a-protocol.org/latest/topics/streaming-and-async/
- Tyler Streaming Example: `packages/tyler/examples/004_streaming.py`

## Requirements
- Must stream tokens to A2A clients as they arrive from the LLM
- Must use the A2A protocol's `TaskArtifactUpdateEvent` with `append` and `lastChunk` fields
- Must support multi-turn tool use (streaming resumes after tool execution)
- Must allow server operators to disable streaming if desired
- Must not break existing non-streaming A2A functionality

## Acceptance Criteria
- Given a client connected via `message/stream`, when the LLM generates tokens, then the client receives `TaskArtifactUpdateEvent`s with partial content within 100ms of generation
- Given a client using non-streaming `message/send`, when the agent completes, then the client receives the full response as before (no regression)
- Given streaming is disabled in server config, when a client connects, then the server behaves as it does today (full response only)
- Given the agent uses tools during execution, when tool execution completes, then streaming resumes for the subsequent LLM response
- Given an error during streaming, then the server emits `TaskStatusUpdateEvent` with `failed` state and closes the stream cleanly

## Non-Goals
- Push notification streaming (already handled by SDK infrastructure)
- Client-side streaming implementation changes (client already supports it)
- Batching/throttling optimization (future enhancement)
- Streaming of tool execution progress (future enhancement)

